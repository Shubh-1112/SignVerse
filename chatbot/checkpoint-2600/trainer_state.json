{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.5874710501045021,
  "eval_steps": 500,
  "global_step": 2600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0022595040388634696,
      "grad_norm": 2.9746131896972656,
      "learning_rate": 0.0001964,
      "loss": 2.339,
      "step": 10
    },
    {
      "epoch": 0.004519008077726939,
      "grad_norm": 3.1051104068756104,
      "learning_rate": 0.00019240000000000001,
      "loss": 2.0914,
      "step": 20
    },
    {
      "epoch": 0.006778512116590409,
      "grad_norm": 2.7272989749908447,
      "learning_rate": 0.0001884,
      "loss": 2.0981,
      "step": 30
    },
    {
      "epoch": 0.009038016155453878,
      "grad_norm": 3.173133611679077,
      "learning_rate": 0.0001844,
      "loss": 2.0151,
      "step": 40
    },
    {
      "epoch": 0.011297520194317347,
      "grad_norm": 3.1552436351776123,
      "learning_rate": 0.00018040000000000002,
      "loss": 1.9682,
      "step": 50
    },
    {
      "epoch": 0.013557024233180818,
      "grad_norm": 3.2962493896484375,
      "learning_rate": 0.0001764,
      "loss": 1.9143,
      "step": 60
    },
    {
      "epoch": 0.015816528272044286,
      "grad_norm": 4.150723934173584,
      "learning_rate": 0.00017240000000000002,
      "loss": 1.7982,
      "step": 70
    },
    {
      "epoch": 0.018076032310907757,
      "grad_norm": 3.6922895908355713,
      "learning_rate": 0.0001684,
      "loss": 1.765,
      "step": 80
    },
    {
      "epoch": 0.020335536349771224,
      "grad_norm": 3.857020378112793,
      "learning_rate": 0.0001644,
      "loss": 1.7152,
      "step": 90
    },
    {
      "epoch": 0.022595040388634694,
      "grad_norm": 3.7412896156311035,
      "learning_rate": 0.00016040000000000002,
      "loss": 1.6623,
      "step": 100
    },
    {
      "epoch": 0.024854544427498165,
      "grad_norm": 3.4498910903930664,
      "learning_rate": 0.0001564,
      "loss": 1.6509,
      "step": 110
    },
    {
      "epoch": 0.027114048466361635,
      "grad_norm": 3.350416898727417,
      "learning_rate": 0.00015240000000000002,
      "loss": 1.5702,
      "step": 120
    },
    {
      "epoch": 0.029373552505225102,
      "grad_norm": 3.900669813156128,
      "learning_rate": 0.0001484,
      "loss": 1.5445,
      "step": 130
    },
    {
      "epoch": 0.03163305654408857,
      "grad_norm": 3.8852450847625732,
      "learning_rate": 0.0001444,
      "loss": 1.5368,
      "step": 140
    },
    {
      "epoch": 0.03389256058295204,
      "grad_norm": 3.8384597301483154,
      "learning_rate": 0.0001404,
      "loss": 1.4372,
      "step": 150
    },
    {
      "epoch": 0.03615206462181551,
      "grad_norm": 3.9781649112701416,
      "learning_rate": 0.0001364,
      "loss": 1.4711,
      "step": 160
    },
    {
      "epoch": 0.038411568660678984,
      "grad_norm": 3.721205949783325,
      "learning_rate": 0.00013240000000000002,
      "loss": 1.3894,
      "step": 170
    },
    {
      "epoch": 0.04067107269954245,
      "grad_norm": 4.4120097160339355,
      "learning_rate": 0.0001284,
      "loss": 1.3826,
      "step": 180
    },
    {
      "epoch": 0.04293057673840592,
      "grad_norm": 3.94600248336792,
      "learning_rate": 0.00012440000000000002,
      "loss": 1.3664,
      "step": 190
    },
    {
      "epoch": 0.04519008077726939,
      "grad_norm": 4.4035115242004395,
      "learning_rate": 0.0001204,
      "loss": 1.3629,
      "step": 200
    },
    {
      "epoch": 0.04744958481613286,
      "grad_norm": 4.200705051422119,
      "learning_rate": 0.0001164,
      "loss": 1.3195,
      "step": 210
    },
    {
      "epoch": 0.04970908885499633,
      "grad_norm": 4.4781174659729,
      "learning_rate": 0.00011240000000000002,
      "loss": 1.3024,
      "step": 220
    },
    {
      "epoch": 0.0519685928938598,
      "grad_norm": 3.8889009952545166,
      "learning_rate": 0.00010840000000000002,
      "loss": 1.2646,
      "step": 230
    },
    {
      "epoch": 0.05422809693272327,
      "grad_norm": 4.073115348815918,
      "learning_rate": 0.0001044,
      "loss": 1.2275,
      "step": 240
    },
    {
      "epoch": 0.056487600971586734,
      "grad_norm": 4.523531913757324,
      "learning_rate": 0.0001004,
      "loss": 1.2535,
      "step": 250
    },
    {
      "epoch": 0.058747105010450204,
      "grad_norm": 3.783203601837158,
      "learning_rate": 9.64e-05,
      "loss": 1.256,
      "step": 260
    },
    {
      "epoch": 0.061006609049313675,
      "grad_norm": 3.999591588973999,
      "learning_rate": 9.240000000000001e-05,
      "loss": 1.2657,
      "step": 270
    },
    {
      "epoch": 0.06326611308817714,
      "grad_norm": 4.108003616333008,
      "learning_rate": 8.840000000000001e-05,
      "loss": 1.2084,
      "step": 280
    },
    {
      "epoch": 0.06552561712704061,
      "grad_norm": 4.117843151092529,
      "learning_rate": 8.44e-05,
      "loss": 1.1753,
      "step": 290
    },
    {
      "epoch": 0.06778512116590409,
      "grad_norm": 4.79267692565918,
      "learning_rate": 8.04e-05,
      "loss": 1.1704,
      "step": 300
    },
    {
      "epoch": 0.07004462520476755,
      "grad_norm": 4.289633750915527,
      "learning_rate": 7.64e-05,
      "loss": 1.1461,
      "step": 310
    },
    {
      "epoch": 0.07230412924363103,
      "grad_norm": 3.7865207195281982,
      "learning_rate": 7.24e-05,
      "loss": 1.142,
      "step": 320
    },
    {
      "epoch": 0.07456363328249449,
      "grad_norm": 4.201812744140625,
      "learning_rate": 6.840000000000001e-05,
      "loss": 1.1386,
      "step": 330
    },
    {
      "epoch": 0.07682313732135797,
      "grad_norm": 3.7121920585632324,
      "learning_rate": 6.440000000000001e-05,
      "loss": 1.1125,
      "step": 340
    },
    {
      "epoch": 0.07908264136022143,
      "grad_norm": 4.301356315612793,
      "learning_rate": 6.04e-05,
      "loss": 1.0604,
      "step": 350
    },
    {
      "epoch": 0.0813421453990849,
      "grad_norm": 3.8974509239196777,
      "learning_rate": 5.6399999999999995e-05,
      "loss": 1.0844,
      "step": 360
    },
    {
      "epoch": 0.08360164943794837,
      "grad_norm": 4.044998645782471,
      "learning_rate": 5.2400000000000007e-05,
      "loss": 1.0641,
      "step": 370
    },
    {
      "epoch": 0.08586115347681184,
      "grad_norm": 4.113840579986572,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 1.1427,
      "step": 380
    },
    {
      "epoch": 0.08812065751567531,
      "grad_norm": 3.823901891708374,
      "learning_rate": 4.44e-05,
      "loss": 1.0911,
      "step": 390
    },
    {
      "epoch": 0.09038016155453878,
      "grad_norm": 4.213931560516357,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 1.0694,
      "step": 400
    },
    {
      "epoch": 0.09263966559340225,
      "grad_norm": 3.8820102214813232,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 1.0574,
      "step": 410
    },
    {
      "epoch": 0.09489916963226572,
      "grad_norm": 4.156405925750732,
      "learning_rate": 3.24e-05,
      "loss": 0.9807,
      "step": 420
    },
    {
      "epoch": 0.09715867367112918,
      "grad_norm": 3.733433246612549,
      "learning_rate": 2.84e-05,
      "loss": 1.0467,
      "step": 430
    },
    {
      "epoch": 0.09941817770999266,
      "grad_norm": 3.841395854949951,
      "learning_rate": 2.44e-05,
      "loss": 1.0453,
      "step": 440
    },
    {
      "epoch": 0.10167768174885612,
      "grad_norm": 4.0524797439575195,
      "learning_rate": 2.04e-05,
      "loss": 1.0291,
      "step": 450
    },
    {
      "epoch": 0.1039371857877196,
      "grad_norm": 5.6195783615112305,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 1.0318,
      "step": 460
    },
    {
      "epoch": 0.10619668982658306,
      "grad_norm": 3.753551721572876,
      "learning_rate": 1.24e-05,
      "loss": 1.049,
      "step": 470
    },
    {
      "epoch": 0.10845619386544654,
      "grad_norm": 4.149322032928467,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.9723,
      "step": 480
    },
    {
      "epoch": 0.11071569790431,
      "grad_norm": 3.7826144695281982,
      "learning_rate": 4.4e-06,
      "loss": 1.06,
      "step": 490
    },
    {
      "epoch": 0.11297520194317347,
      "grad_norm": 4.01225471496582,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.9867,
      "step": 500
    },
    {
      "epoch": 0.11523470598203694,
      "grad_norm": 4.8744635581970215,
      "learning_rate": 0.00010745454545454545,
      "loss": 1.0359,
      "step": 510
    },
    {
      "epoch": 0.11749421002090041,
      "grad_norm": 5.146463394165039,
      "learning_rate": 0.00010563636363636365,
      "loss": 1.0563,
      "step": 520
    },
    {
      "epoch": 0.11975371405976389,
      "grad_norm": 4.008964538574219,
      "learning_rate": 0.00010381818181818181,
      "loss": 1.0405,
      "step": 530
    },
    {
      "epoch": 0.12201321809862735,
      "grad_norm": 4.947887897491455,
      "learning_rate": 0.00010200000000000001,
      "loss": 1.0598,
      "step": 540
    },
    {
      "epoch": 0.12427272213749083,
      "grad_norm": 3.6052446365356445,
      "learning_rate": 0.00010018181818181818,
      "loss": 1.053,
      "step": 550
    },
    {
      "epoch": 0.1265322261763543,
      "grad_norm": 5.297598361968994,
      "learning_rate": 9.836363636363637e-05,
      "loss": 1.0273,
      "step": 560
    },
    {
      "epoch": 0.12879173021521775,
      "grad_norm": 4.955194473266602,
      "learning_rate": 9.654545454545454e-05,
      "loss": 1.0006,
      "step": 570
    },
    {
      "epoch": 0.13105123425408122,
      "grad_norm": 4.966073513031006,
      "learning_rate": 9.472727272727273e-05,
      "loss": 1.002,
      "step": 580
    },
    {
      "epoch": 0.1333107382929447,
      "grad_norm": 4.2466888427734375,
      "learning_rate": 9.290909090909091e-05,
      "loss": 1.0446,
      "step": 590
    },
    {
      "epoch": 0.13557024233180817,
      "grad_norm": 4.771317481994629,
      "learning_rate": 9.109090909090909e-05,
      "loss": 1.0116,
      "step": 600
    },
    {
      "epoch": 0.13782974637067164,
      "grad_norm": 3.961749792098999,
      "learning_rate": 8.927272727272728e-05,
      "loss": 1.0346,
      "step": 610
    },
    {
      "epoch": 0.1400892504095351,
      "grad_norm": 4.442174911499023,
      "learning_rate": 8.745454545454546e-05,
      "loss": 1.0397,
      "step": 620
    },
    {
      "epoch": 0.1423487544483986,
      "grad_norm": 4.23162317276001,
      "learning_rate": 8.563636363636363e-05,
      "loss": 0.9861,
      "step": 630
    },
    {
      "epoch": 0.14460825848726205,
      "grad_norm": 5.189116477966309,
      "learning_rate": 8.381818181818182e-05,
      "loss": 0.99,
      "step": 640
    },
    {
      "epoch": 0.14686776252612552,
      "grad_norm": 4.451313495635986,
      "learning_rate": 8.2e-05,
      "loss": 0.9651,
      "step": 650
    },
    {
      "epoch": 0.14912726656498898,
      "grad_norm": 4.136100769042969,
      "learning_rate": 8.018181818181818e-05,
      "loss": 1.0042,
      "step": 660
    },
    {
      "epoch": 0.15138677060385244,
      "grad_norm": 4.087911128997803,
      "learning_rate": 7.836363636363637e-05,
      "loss": 1.016,
      "step": 670
    },
    {
      "epoch": 0.15364627464271594,
      "grad_norm": 3.5961849689483643,
      "learning_rate": 7.654545454545456e-05,
      "loss": 0.9705,
      "step": 680
    },
    {
      "epoch": 0.1559057786815794,
      "grad_norm": 5.270844459533691,
      "learning_rate": 7.472727272727274e-05,
      "loss": 0.9159,
      "step": 690
    },
    {
      "epoch": 0.15816528272044286,
      "grad_norm": 4.200778007507324,
      "learning_rate": 7.290909090909091e-05,
      "loss": 0.9622,
      "step": 700
    },
    {
      "epoch": 0.16042478675930633,
      "grad_norm": 5.9921135902404785,
      "learning_rate": 7.109090909090909e-05,
      "loss": 0.9674,
      "step": 710
    },
    {
      "epoch": 0.1626842907981698,
      "grad_norm": 4.204482555389404,
      "learning_rate": 6.927272727272728e-05,
      "loss": 0.9463,
      "step": 720
    },
    {
      "epoch": 0.16494379483703328,
      "grad_norm": 4.323162078857422,
      "learning_rate": 6.745454545454546e-05,
      "loss": 0.8889,
      "step": 730
    },
    {
      "epoch": 0.16720329887589674,
      "grad_norm": 4.837227821350098,
      "learning_rate": 6.563636363636364e-05,
      "loss": 0.9708,
      "step": 740
    },
    {
      "epoch": 0.1694628029147602,
      "grad_norm": 4.179494380950928,
      "learning_rate": 6.381818181818183e-05,
      "loss": 0.9337,
      "step": 750
    },
    {
      "epoch": 0.17172230695362367,
      "grad_norm": 4.020084381103516,
      "learning_rate": 6.2e-05,
      "loss": 0.9274,
      "step": 760
    },
    {
      "epoch": 0.17398181099248716,
      "grad_norm": 4.047507286071777,
      "learning_rate": 6.0181818181818187e-05,
      "loss": 0.9435,
      "step": 770
    },
    {
      "epoch": 0.17624131503135063,
      "grad_norm": 3.995037078857422,
      "learning_rate": 5.8363636363636364e-05,
      "loss": 0.941,
      "step": 780
    },
    {
      "epoch": 0.1785008190702141,
      "grad_norm": 4.098938465118408,
      "learning_rate": 5.654545454545455e-05,
      "loss": 0.9298,
      "step": 790
    },
    {
      "epoch": 0.18076032310907755,
      "grad_norm": 4.016592979431152,
      "learning_rate": 5.4727272727272724e-05,
      "loss": 0.9118,
      "step": 800
    },
    {
      "epoch": 0.18301982714794102,
      "grad_norm": 3.3519153594970703,
      "learning_rate": 5.290909090909091e-05,
      "loss": 0.9059,
      "step": 810
    },
    {
      "epoch": 0.1852793311868045,
      "grad_norm": 3.7398266792297363,
      "learning_rate": 5.109090909090909e-05,
      "loss": 0.9062,
      "step": 820
    },
    {
      "epoch": 0.18753883522566797,
      "grad_norm": 4.219515323638916,
      "learning_rate": 4.9272727272727276e-05,
      "loss": 0.8541,
      "step": 830
    },
    {
      "epoch": 0.18979833926453143,
      "grad_norm": 4.391800880432129,
      "learning_rate": 4.745454545454546e-05,
      "loss": 0.9504,
      "step": 840
    },
    {
      "epoch": 0.1920578433033949,
      "grad_norm": 3.238372564315796,
      "learning_rate": 4.563636363636364e-05,
      "loss": 0.9304,
      "step": 850
    },
    {
      "epoch": 0.19431734734225836,
      "grad_norm": 4.366544246673584,
      "learning_rate": 4.381818181818182e-05,
      "loss": 0.8645,
      "step": 860
    },
    {
      "epoch": 0.19657685138112185,
      "grad_norm": 3.7507784366607666,
      "learning_rate": 4.2e-05,
      "loss": 0.8947,
      "step": 870
    },
    {
      "epoch": 0.19883635541998532,
      "grad_norm": 3.385470151901245,
      "learning_rate": 4.018181818181818e-05,
      "loss": 0.9105,
      "step": 880
    },
    {
      "epoch": 0.20109585945884878,
      "grad_norm": 3.5721993446350098,
      "learning_rate": 3.8363636363636365e-05,
      "loss": 0.8886,
      "step": 890
    },
    {
      "epoch": 0.20335536349771224,
      "grad_norm": 3.827413320541382,
      "learning_rate": 3.654545454545455e-05,
      "loss": 0.9028,
      "step": 900
    },
    {
      "epoch": 0.20561486753657574,
      "grad_norm": 4.378591060638428,
      "learning_rate": 3.472727272727273e-05,
      "loss": 0.8652,
      "step": 910
    },
    {
      "epoch": 0.2078743715754392,
      "grad_norm": 4.15671968460083,
      "learning_rate": 3.290909090909091e-05,
      "loss": 0.8989,
      "step": 920
    },
    {
      "epoch": 0.21013387561430266,
      "grad_norm": 3.616581678390503,
      "learning_rate": 3.1090909090909094e-05,
      "loss": 0.8654,
      "step": 930
    },
    {
      "epoch": 0.21239337965316613,
      "grad_norm": 3.493055820465088,
      "learning_rate": 2.9272727272727274e-05,
      "loss": 0.8481,
      "step": 940
    },
    {
      "epoch": 0.2146528836920296,
      "grad_norm": 3.4878807067871094,
      "learning_rate": 2.7454545454545455e-05,
      "loss": 0.8817,
      "step": 950
    },
    {
      "epoch": 0.21691238773089308,
      "grad_norm": 3.4185526371002197,
      "learning_rate": 2.5636363636363635e-05,
      "loss": 0.841,
      "step": 960
    },
    {
      "epoch": 0.21917189176975654,
      "grad_norm": 3.736966371536255,
      "learning_rate": 2.381818181818182e-05,
      "loss": 0.8865,
      "step": 970
    },
    {
      "epoch": 0.22143139580862,
      "grad_norm": 4.115604877471924,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.8807,
      "step": 980
    },
    {
      "epoch": 0.22369089984748347,
      "grad_norm": 4.391178131103516,
      "learning_rate": 2.0181818181818183e-05,
      "loss": 0.8196,
      "step": 990
    },
    {
      "epoch": 0.22595040388634693,
      "grad_norm": 3.390124559402466,
      "learning_rate": 1.8363636363636364e-05,
      "loss": 0.8518,
      "step": 1000
    },
    {
      "epoch": 0.22820990792521043,
      "grad_norm": 3.440969705581665,
      "learning_rate": 1.6545454545454548e-05,
      "loss": 0.8817,
      "step": 1010
    },
    {
      "epoch": 0.2304694119640739,
      "grad_norm": 3.7356910705566406,
      "learning_rate": 1.4727272727272728e-05,
      "loss": 0.8585,
      "step": 1020
    },
    {
      "epoch": 0.23272891600293735,
      "grad_norm": 3.4114608764648438,
      "learning_rate": 1.290909090909091e-05,
      "loss": 0.8684,
      "step": 1030
    },
    {
      "epoch": 0.23498842004180082,
      "grad_norm": 3.625333070755005,
      "learning_rate": 1.1090909090909092e-05,
      "loss": 0.8128,
      "step": 1040
    },
    {
      "epoch": 0.2372479240806643,
      "grad_norm": 4.024484634399414,
      "learning_rate": 9.272727272727273e-06,
      "loss": 0.8674,
      "step": 1050
    },
    {
      "epoch": 0.23950742811952777,
      "grad_norm": 4.383498668670654,
      "learning_rate": 7.454545454545454e-06,
      "loss": 0.822,
      "step": 1060
    },
    {
      "epoch": 0.24176693215839123,
      "grad_norm": 4.41750431060791,
      "learning_rate": 5.636363636363637e-06,
      "loss": 0.872,
      "step": 1070
    },
    {
      "epoch": 0.2440264361972547,
      "grad_norm": 3.3912417888641357,
      "learning_rate": 3.818181818181819e-06,
      "loss": 0.798,
      "step": 1080
    },
    {
      "epoch": 0.24628594023611816,
      "grad_norm": 3.5117790699005127,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.8718,
      "step": 1090
    },
    {
      "epoch": 0.24854544427498165,
      "grad_norm": 4.067563056945801,
      "learning_rate": 1.8181818181818183e-07,
      "loss": 0.8419,
      "step": 1100
    },
    {
      "epoch": 0.2508049483138451,
      "grad_norm": 4.025784015655518,
      "learning_rate": 4.446002805049089e-05,
      "loss": 0.8872,
      "step": 1110
    },
    {
      "epoch": 0.2530644523527086,
      "grad_norm": 3.4503066539764404,
      "learning_rate": 4.305750350631136e-05,
      "loss": 0.8318,
      "step": 1120
    },
    {
      "epoch": 0.25532395639157207,
      "grad_norm": 3.5248019695281982,
      "learning_rate": 4.165497896213184e-05,
      "loss": 0.8409,
      "step": 1130
    },
    {
      "epoch": 0.2575834604304355,
      "grad_norm": 3.9693641662597656,
      "learning_rate": 4.0252454417952316e-05,
      "loss": 0.8388,
      "step": 1140
    },
    {
      "epoch": 0.259842964469299,
      "grad_norm": 4.364158630371094,
      "learning_rate": 3.8849929873772796e-05,
      "loss": 0.841,
      "step": 1150
    },
    {
      "epoch": 0.26210246850816243,
      "grad_norm": 3.483929395675659,
      "learning_rate": 3.744740532959327e-05,
      "loss": 0.843,
      "step": 1160
    },
    {
      "epoch": 0.2643619725470259,
      "grad_norm": 3.5813887119293213,
      "learning_rate": 3.604488078541375e-05,
      "loss": 0.8409,
      "step": 1170
    },
    {
      "epoch": 0.2666214765858894,
      "grad_norm": 3.4083542823791504,
      "learning_rate": 3.464235624123422e-05,
      "loss": 0.8226,
      "step": 1180
    },
    {
      "epoch": 0.26888098062475285,
      "grad_norm": 3.852962017059326,
      "learning_rate": 3.32398316970547e-05,
      "loss": 0.8433,
      "step": 1190
    },
    {
      "epoch": 0.27114048466361634,
      "grad_norm": 3.6649603843688965,
      "learning_rate": 3.1837307152875176e-05,
      "loss": 0.8167,
      "step": 1200
    },
    {
      "epoch": 0.2733999887024798,
      "grad_norm": 3.814215660095215,
      "learning_rate": 3.0434782608695656e-05,
      "loss": 0.8654,
      "step": 1210
    },
    {
      "epoch": 0.27565949274134327,
      "grad_norm": 5.088432312011719,
      "learning_rate": 2.9032258064516133e-05,
      "loss": 0.8833,
      "step": 1220
    },
    {
      "epoch": 0.27791899678020676,
      "grad_norm": 3.7252538204193115,
      "learning_rate": 2.762973352033661e-05,
      "loss": 0.8113,
      "step": 1230
    },
    {
      "epoch": 0.2801785008190702,
      "grad_norm": 3.668503522872925,
      "learning_rate": 2.6227208976157086e-05,
      "loss": 0.8215,
      "step": 1240
    },
    {
      "epoch": 0.2824380048579337,
      "grad_norm": 3.5946691036224365,
      "learning_rate": 2.482468443197756e-05,
      "loss": 0.8437,
      "step": 1250
    },
    {
      "epoch": 0.2846975088967972,
      "grad_norm": 3.485302209854126,
      "learning_rate": 2.3422159887798036e-05,
      "loss": 0.8395,
      "step": 1260
    },
    {
      "epoch": 0.2869570129356606,
      "grad_norm": 3.8159046173095703,
      "learning_rate": 2.2019635343618513e-05,
      "loss": 0.8433,
      "step": 1270
    },
    {
      "epoch": 0.2892165169745241,
      "grad_norm": 4.037877082824707,
      "learning_rate": 2.061711079943899e-05,
      "loss": 0.8372,
      "step": 1280
    },
    {
      "epoch": 0.29147602101338754,
      "grad_norm": 4.030433177947998,
      "learning_rate": 1.9214586255259466e-05,
      "loss": 0.8611,
      "step": 1290
    },
    {
      "epoch": 0.29373552505225103,
      "grad_norm": 3.5956203937530518,
      "learning_rate": 1.7812061711079943e-05,
      "loss": 0.8433,
      "step": 1300
    },
    {
      "epoch": 0.2959950290911145,
      "grad_norm": 3.4607045650482178,
      "learning_rate": 1.640953716690042e-05,
      "loss": 0.8239,
      "step": 1310
    },
    {
      "epoch": 0.29825453312997796,
      "grad_norm": 3.279341220855713,
      "learning_rate": 1.5007012622720898e-05,
      "loss": 0.8478,
      "step": 1320
    },
    {
      "epoch": 0.30051403716884145,
      "grad_norm": 3.799170732498169,
      "learning_rate": 1.3604488078541374e-05,
      "loss": 0.8174,
      "step": 1330
    },
    {
      "epoch": 0.3027735412077049,
      "grad_norm": 4.336767673492432,
      "learning_rate": 1.2201963534361851e-05,
      "loss": 0.7993,
      "step": 1340
    },
    {
      "epoch": 0.3050330452465684,
      "grad_norm": 3.584012031555176,
      "learning_rate": 1.0799438990182328e-05,
      "loss": 0.7998,
      "step": 1350
    },
    {
      "epoch": 0.30729254928543187,
      "grad_norm": 3.718494415283203,
      "learning_rate": 9.396914446002806e-06,
      "loss": 0.788,
      "step": 1360
    },
    {
      "epoch": 0.3095520533242953,
      "grad_norm": 3.5008749961853027,
      "learning_rate": 7.994389901823283e-06,
      "loss": 0.7936,
      "step": 1370
    },
    {
      "epoch": 0.3118115573631588,
      "grad_norm": 3.112989664077759,
      "learning_rate": 6.5918653576437585e-06,
      "loss": 0.8074,
      "step": 1380
    },
    {
      "epoch": 0.31407106140202223,
      "grad_norm": 3.7223398685455322,
      "learning_rate": 5.189340813464236e-06,
      "loss": 0.8116,
      "step": 1390
    },
    {
      "epoch": 0.3163305654408857,
      "grad_norm": 3.3964622020721436,
      "learning_rate": 3.7868162692847127e-06,
      "loss": 0.8195,
      "step": 1400
    },
    {
      "epoch": 0.3185900694797492,
      "grad_norm": 3.6918818950653076,
      "learning_rate": 2.3842917251051894e-06,
      "loss": 0.8337,
      "step": 1410
    },
    {
      "epoch": 0.32084957351861265,
      "grad_norm": 5.107503890991211,
      "learning_rate": 9.817671809256662e-07,
      "loss": 0.8346,
      "step": 1420
    },
    {
      "epoch": 0.32310907755747614,
      "grad_norm": 3.6348912715911865,
      "learning_rate": 5.8933859822309976e-05,
      "loss": 0.8119,
      "step": 1430
    },
    {
      "epoch": 0.3253685815963396,
      "grad_norm": 4.7959489822387695,
      "learning_rate": 5.7946692991115505e-05,
      "loss": 0.8241,
      "step": 1440
    },
    {
      "epoch": 0.32762808563520307,
      "grad_norm": 4.030542850494385,
      "learning_rate": 5.695952615992103e-05,
      "loss": 0.8483,
      "step": 1450
    },
    {
      "epoch": 0.32988758967406656,
      "grad_norm": 3.8045506477355957,
      "learning_rate": 5.5972359328726556e-05,
      "loss": 0.8385,
      "step": 1460
    },
    {
      "epoch": 0.33214709371293,
      "grad_norm": 3.9790053367614746,
      "learning_rate": 5.498519249753209e-05,
      "loss": 0.8459,
      "step": 1470
    },
    {
      "epoch": 0.3344065977517935,
      "grad_norm": 3.337247610092163,
      "learning_rate": 5.399802566633761e-05,
      "loss": 0.8598,
      "step": 1480
    },
    {
      "epoch": 0.3366661017906569,
      "grad_norm": 3.8021206855773926,
      "learning_rate": 5.301085883514314e-05,
      "loss": 0.8118,
      "step": 1490
    },
    {
      "epoch": 0.3389256058295204,
      "grad_norm": 3.9930474758148193,
      "learning_rate": 5.2023692003948664e-05,
      "loss": 0.8124,
      "step": 1500
    },
    {
      "epoch": 0.3411851098683839,
      "grad_norm": 3.5702922344207764,
      "learning_rate": 5.10365251727542e-05,
      "loss": 0.7887,
      "step": 1510
    },
    {
      "epoch": 0.34344461390724734,
      "grad_norm": 3.7539541721343994,
      "learning_rate": 5.004935834155973e-05,
      "loss": 0.8391,
      "step": 1520
    },
    {
      "epoch": 0.34570411794611083,
      "grad_norm": 3.5830743312835693,
      "learning_rate": 4.906219151036525e-05,
      "loss": 0.8252,
      "step": 1530
    },
    {
      "epoch": 0.3479636219849743,
      "grad_norm": 3.866184711456299,
      "learning_rate": 4.8075024679170785e-05,
      "loss": 0.8227,
      "step": 1540
    },
    {
      "epoch": 0.35022312602383776,
      "grad_norm": 4.511010646820068,
      "learning_rate": 4.708785784797631e-05,
      "loss": 0.8546,
      "step": 1550
    },
    {
      "epoch": 0.35248263006270125,
      "grad_norm": 3.6395270824432373,
      "learning_rate": 4.6100691016781836e-05,
      "loss": 0.8396,
      "step": 1560
    },
    {
      "epoch": 0.3547421341015647,
      "grad_norm": 3.0756473541259766,
      "learning_rate": 4.511352418558737e-05,
      "loss": 0.8021,
      "step": 1570
    },
    {
      "epoch": 0.3570016381404282,
      "grad_norm": 3.5177319049835205,
      "learning_rate": 4.4126357354392894e-05,
      "loss": 0.8194,
      "step": 1580
    },
    {
      "epoch": 0.35926114217929167,
      "grad_norm": 4.09970235824585,
      "learning_rate": 4.313919052319842e-05,
      "loss": 0.8198,
      "step": 1590
    },
    {
      "epoch": 0.3615206462181551,
      "grad_norm": 3.7017385959625244,
      "learning_rate": 4.215202369200395e-05,
      "loss": 0.8933,
      "step": 1600
    },
    {
      "epoch": 0.3637801502570186,
      "grad_norm": 3.900709390640259,
      "learning_rate": 4.116485686080948e-05,
      "loss": 0.8337,
      "step": 1610
    },
    {
      "epoch": 0.36603965429588203,
      "grad_norm": 4.663597583770752,
      "learning_rate": 4.017769002961501e-05,
      "loss": 0.8213,
      "step": 1620
    },
    {
      "epoch": 0.3682991583347455,
      "grad_norm": 3.8877816200256348,
      "learning_rate": 3.919052319842054e-05,
      "loss": 0.8205,
      "step": 1630
    },
    {
      "epoch": 0.370558662373609,
      "grad_norm": 3.8734748363494873,
      "learning_rate": 3.820335636722606e-05,
      "loss": 0.8566,
      "step": 1640
    },
    {
      "epoch": 0.37281816641247245,
      "grad_norm": 3.9213478565216064,
      "learning_rate": 3.7216189536031595e-05,
      "loss": 0.8265,
      "step": 1650
    },
    {
      "epoch": 0.37507767045133594,
      "grad_norm": 4.521200180053711,
      "learning_rate": 3.622902270483712e-05,
      "loss": 0.8829,
      "step": 1660
    },
    {
      "epoch": 0.3773371744901994,
      "grad_norm": 4.786375999450684,
      "learning_rate": 3.5241855873642646e-05,
      "loss": 0.7973,
      "step": 1670
    },
    {
      "epoch": 0.37959667852906287,
      "grad_norm": 4.703478813171387,
      "learning_rate": 3.4254689042448174e-05,
      "loss": 0.8315,
      "step": 1680
    },
    {
      "epoch": 0.38185618256792636,
      "grad_norm": 3.490060806274414,
      "learning_rate": 3.32675222112537e-05,
      "loss": 0.8049,
      "step": 1690
    },
    {
      "epoch": 0.3841156866067898,
      "grad_norm": 4.0514116287231445,
      "learning_rate": 3.228035538005923e-05,
      "loss": 0.8468,
      "step": 1700
    },
    {
      "epoch": 0.3863751906456533,
      "grad_norm": 4.440121173858643,
      "learning_rate": 3.129318854886476e-05,
      "loss": 0.8016,
      "step": 1710
    },
    {
      "epoch": 0.3886346946845167,
      "grad_norm": 4.1057610511779785,
      "learning_rate": 3.0306021717670286e-05,
      "loss": 0.7713,
      "step": 1720
    },
    {
      "epoch": 0.3908941987233802,
      "grad_norm": 3.7009975910186768,
      "learning_rate": 2.9318854886475815e-05,
      "loss": 0.8044,
      "step": 1730
    },
    {
      "epoch": 0.3931537027622437,
      "grad_norm": 4.083148002624512,
      "learning_rate": 2.833168805528134e-05,
      "loss": 0.8164,
      "step": 1740
    },
    {
      "epoch": 0.39541320680110714,
      "grad_norm": 3.6540863513946533,
      "learning_rate": 2.7344521224086872e-05,
      "loss": 0.7876,
      "step": 1750
    },
    {
      "epoch": 0.39767271083997063,
      "grad_norm": 4.189210414886475,
      "learning_rate": 2.6357354392892404e-05,
      "loss": 0.8107,
      "step": 1760
    },
    {
      "epoch": 0.39993221487883407,
      "grad_norm": 4.075258731842041,
      "learning_rate": 2.5370187561697926e-05,
      "loss": 0.7986,
      "step": 1770
    },
    {
      "epoch": 0.40219171891769756,
      "grad_norm": 4.310105800628662,
      "learning_rate": 2.4383020730503455e-05,
      "loss": 0.7959,
      "step": 1780
    },
    {
      "epoch": 0.40445122295656105,
      "grad_norm": 3.60201358795166,
      "learning_rate": 2.3395853899308984e-05,
      "loss": 0.7934,
      "step": 1790
    },
    {
      "epoch": 0.4067107269954245,
      "grad_norm": 4.337475299835205,
      "learning_rate": 2.2408687068114512e-05,
      "loss": 0.8251,
      "step": 1800
    },
    {
      "epoch": 0.408970231034288,
      "grad_norm": 3.505478858947754,
      "learning_rate": 2.142152023692004e-05,
      "loss": 0.7593,
      "step": 1810
    },
    {
      "epoch": 0.41122973507315147,
      "grad_norm": 3.5846199989318848,
      "learning_rate": 2.0434353405725567e-05,
      "loss": 0.8051,
      "step": 1820
    },
    {
      "epoch": 0.4134892391120149,
      "grad_norm": 4.4000244140625,
      "learning_rate": 1.9447186574531095e-05,
      "loss": 0.7899,
      "step": 1830
    },
    {
      "epoch": 0.4157487431508784,
      "grad_norm": 3.981184720993042,
      "learning_rate": 1.8460019743336624e-05,
      "loss": 0.8384,
      "step": 1840
    },
    {
      "epoch": 0.41800824718974183,
      "grad_norm": 3.2776143550872803,
      "learning_rate": 1.7472852912142153e-05,
      "loss": 0.7924,
      "step": 1850
    },
    {
      "epoch": 0.4202677512286053,
      "grad_norm": 4.035171985626221,
      "learning_rate": 1.648568608094768e-05,
      "loss": 0.7789,
      "step": 1860
    },
    {
      "epoch": 0.4225272552674688,
      "grad_norm": 3.285731554031372,
      "learning_rate": 1.549851924975321e-05,
      "loss": 0.7786,
      "step": 1870
    },
    {
      "epoch": 0.42478675930633225,
      "grad_norm": 4.046950817108154,
      "learning_rate": 1.4511352418558737e-05,
      "loss": 0.8598,
      "step": 1880
    },
    {
      "epoch": 0.42704626334519574,
      "grad_norm": 3.5274806022644043,
      "learning_rate": 1.3524185587364266e-05,
      "loss": 0.751,
      "step": 1890
    },
    {
      "epoch": 0.4293057673840592,
      "grad_norm": 4.183573246002197,
      "learning_rate": 1.2537018756169793e-05,
      "loss": 0.7607,
      "step": 1900
    },
    {
      "epoch": 0.43156527142292267,
      "grad_norm": 4.799456596374512,
      "learning_rate": 1.1549851924975322e-05,
      "loss": 0.8254,
      "step": 1910
    },
    {
      "epoch": 0.43382477546178616,
      "grad_norm": 3.053222417831421,
      "learning_rate": 1.0562685093780849e-05,
      "loss": 0.8027,
      "step": 1920
    },
    {
      "epoch": 0.4360842795006496,
      "grad_norm": 3.979550361633301,
      "learning_rate": 9.575518262586378e-06,
      "loss": 0.7712,
      "step": 1930
    },
    {
      "epoch": 0.4383437835395131,
      "grad_norm": 3.7309799194335938,
      "learning_rate": 8.588351431391906e-06,
      "loss": 0.7615,
      "step": 1940
    },
    {
      "epoch": 0.4406032875783765,
      "grad_norm": 4.219143867492676,
      "learning_rate": 7.6011846001974334e-06,
      "loss": 0.7922,
      "step": 1950
    },
    {
      "epoch": 0.44286279161724,
      "grad_norm": 3.750075101852417,
      "learning_rate": 6.614017769002961e-06,
      "loss": 0.7661,
      "step": 1960
    },
    {
      "epoch": 0.4451222956561035,
      "grad_norm": 3.6777591705322266,
      "learning_rate": 5.62685093780849e-06,
      "loss": 0.7676,
      "step": 1970
    },
    {
      "epoch": 0.44738179969496694,
      "grad_norm": 4.469845771789551,
      "learning_rate": 4.639684106614018e-06,
      "loss": 0.7902,
      "step": 1980
    },
    {
      "epoch": 0.44964130373383043,
      "grad_norm": 3.7469465732574463,
      "learning_rate": 3.652517275419546e-06,
      "loss": 0.7747,
      "step": 1990
    },
    {
      "epoch": 0.45190080777269387,
      "grad_norm": 3.30418062210083,
      "learning_rate": 2.665350444225074e-06,
      "loss": 0.7871,
      "step": 2000
    },
    {
      "epoch": 0.45416031181155736,
      "grad_norm": 3.519857168197632,
      "learning_rate": 1.678183613030602e-06,
      "loss": 0.7989,
      "step": 2010
    },
    {
      "epoch": 0.45641981585042085,
      "grad_norm": 3.875614643096924,
      "learning_rate": 6.910167818361304e-07,
      "loss": 0.7757,
      "step": 2020
    },
    {
      "epoch": 0.4586793198892843,
      "grad_norm": 3.8056540489196777,
      "learning_rate": 5.113719735876743e-05,
      "loss": 0.7544,
      "step": 2030
    },
    {
      "epoch": 0.4609388239281478,
      "grad_norm": 4.34370756149292,
      "learning_rate": 5.0403521643433606e-05,
      "loss": 0.7423,
      "step": 2040
    },
    {
      "epoch": 0.4631983279670112,
      "grad_norm": 3.323345899581909,
      "learning_rate": 4.966984592809979e-05,
      "loss": 0.8036,
      "step": 2050
    },
    {
      "epoch": 0.4654578320058747,
      "grad_norm": 3.870548963546753,
      "learning_rate": 4.893617021276596e-05,
      "loss": 0.764,
      "step": 2060
    },
    {
      "epoch": 0.4677173360447382,
      "grad_norm": 4.415432453155518,
      "learning_rate": 4.820249449743214e-05,
      "loss": 0.7615,
      "step": 2070
    },
    {
      "epoch": 0.46997684008360163,
      "grad_norm": 4.125053405761719,
      "learning_rate": 4.7468818782098315e-05,
      "loss": 0.7907,
      "step": 2080
    },
    {
      "epoch": 0.4722363441224651,
      "grad_norm": 3.060269355773926,
      "learning_rate": 4.673514306676449e-05,
      "loss": 0.8036,
      "step": 2090
    },
    {
      "epoch": 0.4744958481613286,
      "grad_norm": 2.85994291305542,
      "learning_rate": 4.600146735143067e-05,
      "loss": 0.7735,
      "step": 2100
    },
    {
      "epoch": 0.47675535220019205,
      "grad_norm": 3.565854549407959,
      "learning_rate": 4.526779163609684e-05,
      "loss": 0.824,
      "step": 2110
    },
    {
      "epoch": 0.47901485623905554,
      "grad_norm": 3.581677198410034,
      "learning_rate": 4.4534115920763024e-05,
      "loss": 0.7753,
      "step": 2120
    },
    {
      "epoch": 0.481274360277919,
      "grad_norm": 4.502410411834717,
      "learning_rate": 4.3800440205429204e-05,
      "loss": 0.858,
      "step": 2130
    },
    {
      "epoch": 0.48353386431678247,
      "grad_norm": 3.820239782333374,
      "learning_rate": 4.306676449009538e-05,
      "loss": 0.7826,
      "step": 2140
    },
    {
      "epoch": 0.48579336835564596,
      "grad_norm": 3.4838225841522217,
      "learning_rate": 4.233308877476156e-05,
      "loss": 0.7908,
      "step": 2150
    },
    {
      "epoch": 0.4880528723945094,
      "grad_norm": 3.865520715713501,
      "learning_rate": 4.159941305942773e-05,
      "loss": 0.814,
      "step": 2160
    },
    {
      "epoch": 0.4903123764333729,
      "grad_norm": 3.588332414627075,
      "learning_rate": 4.086573734409391e-05,
      "loss": 0.8007,
      "step": 2170
    },
    {
      "epoch": 0.4925718804722363,
      "grad_norm": 4.115631103515625,
      "learning_rate": 4.0132061628760093e-05,
      "loss": 0.7996,
      "step": 2180
    },
    {
      "epoch": 0.4948313845110998,
      "grad_norm": 3.993082046508789,
      "learning_rate": 3.939838591342627e-05,
      "loss": 0.7816,
      "step": 2190
    },
    {
      "epoch": 0.4970908885499633,
      "grad_norm": 3.6867003440856934,
      "learning_rate": 3.866471019809245e-05,
      "loss": 0.7579,
      "step": 2200
    },
    {
      "epoch": 0.49935039258882674,
      "grad_norm": 3.652669668197632,
      "learning_rate": 3.793103448275862e-05,
      "loss": 0.7897,
      "step": 2210
    },
    {
      "epoch": 0.5016098966276902,
      "grad_norm": 4.055652141571045,
      "learning_rate": 3.71973587674248e-05,
      "loss": 0.7916,
      "step": 2220
    },
    {
      "epoch": 0.5038694006665537,
      "grad_norm": 4.166863918304443,
      "learning_rate": 3.646368305209098e-05,
      "loss": 0.8092,
      "step": 2230
    },
    {
      "epoch": 0.5061289047054172,
      "grad_norm": 3.9311132431030273,
      "learning_rate": 3.5730007336757156e-05,
      "loss": 0.8131,
      "step": 2240
    },
    {
      "epoch": 0.5083884087442806,
      "grad_norm": 3.843855857849121,
      "learning_rate": 3.499633162142334e-05,
      "loss": 0.8083,
      "step": 2250
    },
    {
      "epoch": 0.5106479127831441,
      "grad_norm": 3.7560439109802246,
      "learning_rate": 3.426265590608951e-05,
      "loss": 0.7559,
      "step": 2260
    },
    {
      "epoch": 0.5129074168220076,
      "grad_norm": 3.6204020977020264,
      "learning_rate": 3.3528980190755685e-05,
      "loss": 0.7648,
      "step": 2270
    },
    {
      "epoch": 0.515166920860871,
      "grad_norm": 3.608944892883301,
      "learning_rate": 3.2795304475421865e-05,
      "loss": 0.7509,
      "step": 2280
    },
    {
      "epoch": 0.5174264248997346,
      "grad_norm": 3.5215306282043457,
      "learning_rate": 3.206162876008804e-05,
      "loss": 0.761,
      "step": 2290
    },
    {
      "epoch": 0.519685928938598,
      "grad_norm": 3.6639747619628906,
      "learning_rate": 3.132795304475422e-05,
      "loss": 0.7759,
      "step": 2300
    },
    {
      "epoch": 0.5219454329774614,
      "grad_norm": 4.011205196380615,
      "learning_rate": 3.059427732942039e-05,
      "loss": 0.806,
      "step": 2310
    },
    {
      "epoch": 0.5242049370163249,
      "grad_norm": 3.558063507080078,
      "learning_rate": 2.9860601614086574e-05,
      "loss": 0.7606,
      "step": 2320
    },
    {
      "epoch": 0.5264644410551884,
      "grad_norm": 3.8076670169830322,
      "learning_rate": 2.9126925898752754e-05,
      "loss": 0.7772,
      "step": 2330
    },
    {
      "epoch": 0.5287239450940519,
      "grad_norm": 3.4039995670318604,
      "learning_rate": 2.8393250183418928e-05,
      "loss": 0.7775,
      "step": 2340
    },
    {
      "epoch": 0.5309834491329153,
      "grad_norm": 4.046455383300781,
      "learning_rate": 2.765957446808511e-05,
      "loss": 0.7783,
      "step": 2350
    },
    {
      "epoch": 0.5332429531717788,
      "grad_norm": 3.887786388397217,
      "learning_rate": 2.6925898752751282e-05,
      "loss": 0.7701,
      "step": 2360
    },
    {
      "epoch": 0.5355024572106423,
      "grad_norm": 3.8059608936309814,
      "learning_rate": 2.6192223037417463e-05,
      "loss": 0.7547,
      "step": 2370
    },
    {
      "epoch": 0.5377619612495057,
      "grad_norm": 3.690701723098755,
      "learning_rate": 2.5458547322083643e-05,
      "loss": 0.7758,
      "step": 2380
    },
    {
      "epoch": 0.5400214652883693,
      "grad_norm": 3.430753231048584,
      "learning_rate": 2.4724871606749817e-05,
      "loss": 0.7724,
      "step": 2390
    },
    {
      "epoch": 0.5422809693272327,
      "grad_norm": 3.7195730209350586,
      "learning_rate": 2.3991195891415998e-05,
      "loss": 0.7854,
      "step": 2400
    },
    {
      "epoch": 0.5445404733660961,
      "grad_norm": 3.6503119468688965,
      "learning_rate": 2.325752017608217e-05,
      "loss": 0.7756,
      "step": 2410
    },
    {
      "epoch": 0.5467999774049596,
      "grad_norm": 3.6147029399871826,
      "learning_rate": 2.252384446074835e-05,
      "loss": 0.8238,
      "step": 2420
    },
    {
      "epoch": 0.5490594814438231,
      "grad_norm": 3.9897356033325195,
      "learning_rate": 2.1790168745414526e-05,
      "loss": 0.7801,
      "step": 2430
    },
    {
      "epoch": 0.5513189854826865,
      "grad_norm": 3.909463405609131,
      "learning_rate": 2.1056493030080703e-05,
      "loss": 0.7834,
      "step": 2440
    },
    {
      "epoch": 0.55357848952155,
      "grad_norm": 4.138208866119385,
      "learning_rate": 2.0322817314746884e-05,
      "loss": 0.7608,
      "step": 2450
    },
    {
      "epoch": 0.5558379935604135,
      "grad_norm": 3.7828357219696045,
      "learning_rate": 1.958914159941306e-05,
      "loss": 0.7438,
      "step": 2460
    },
    {
      "epoch": 0.558097497599277,
      "grad_norm": 3.8214802742004395,
      "learning_rate": 1.8855465884079238e-05,
      "loss": 0.7573,
      "step": 2470
    },
    {
      "epoch": 0.5603570016381404,
      "grad_norm": 3.997650146484375,
      "learning_rate": 1.8121790168745415e-05,
      "loss": 0.7917,
      "step": 2480
    },
    {
      "epoch": 0.5626165056770039,
      "grad_norm": 3.442925453186035,
      "learning_rate": 1.7388114453411596e-05,
      "loss": 0.7747,
      "step": 2490
    },
    {
      "epoch": 0.5648760097158674,
      "grad_norm": 4.000127792358398,
      "learning_rate": 1.6654438738077773e-05,
      "loss": 0.7619,
      "step": 2500
    },
    {
      "epoch": 0.5671355137547308,
      "grad_norm": 3.413421154022217,
      "learning_rate": 1.5920763022743947e-05,
      "loss": 0.7162,
      "step": 2510
    },
    {
      "epoch": 0.5693950177935944,
      "grad_norm": 3.939321756362915,
      "learning_rate": 1.5187087307410125e-05,
      "loss": 0.7728,
      "step": 2520
    },
    {
      "epoch": 0.5716545218324578,
      "grad_norm": 3.469390630722046,
      "learning_rate": 1.4453411592076303e-05,
      "loss": 0.7242,
      "step": 2530
    },
    {
      "epoch": 0.5739140258713212,
      "grad_norm": 3.5969913005828857,
      "learning_rate": 1.3719735876742481e-05,
      "loss": 0.7699,
      "step": 2540
    },
    {
      "epoch": 0.5761735299101847,
      "grad_norm": 4.001410484313965,
      "learning_rate": 1.2986060161408659e-05,
      "loss": 0.7747,
      "step": 2550
    },
    {
      "epoch": 0.5784330339490482,
      "grad_norm": 3.860224723815918,
      "learning_rate": 1.2252384446074836e-05,
      "loss": 0.7468,
      "step": 2560
    },
    {
      "epoch": 0.5806925379879116,
      "grad_norm": 3.887255907058716,
      "learning_rate": 1.1518708730741013e-05,
      "loss": 0.7393,
      "step": 2570
    },
    {
      "epoch": 0.5829520420267751,
      "grad_norm": 3.860377550125122,
      "learning_rate": 1.078503301540719e-05,
      "loss": 0.7273,
      "step": 2580
    },
    {
      "epoch": 0.5852115460656386,
      "grad_norm": 3.9321069717407227,
      "learning_rate": 1.0051357300073367e-05,
      "loss": 0.7283,
      "step": 2590
    },
    {
      "epoch": 0.5874710501045021,
      "grad_norm": 4.440736293792725,
      "learning_rate": 9.317681584739546e-06,
      "loss": 0.7524,
      "step": 2600
    }
  ],
  "logging_steps": 10,
  "max_steps": 2726,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 9.099812574068736e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
